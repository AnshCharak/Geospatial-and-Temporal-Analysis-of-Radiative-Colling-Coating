# -*- coding: utf-8 -*-
"""code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ByN5wcHVEoNMKk7Ojvwgh3yhzlFz09QO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import folium
import plotly.express as px
import geopandas as gpd
import scipy
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from folium.plugins import HeatMap

file_paths = {
    "AM1.5 Spectra Data": "/content/drive/MyDrive/Data/AM1.5_spectra_data.csv",
    "Ambient temperature Data": "/content/drive/MyDrive/Data/Temperature_data.csv",
    "Precipitation Data": "/content/drive/MyDrive/Data/Precipitation_data.csv",
    "Solar Radiation Data": "/content/drive/MyDrive/Data/Solar_radiation_data.csv",
    "Surface Temperature Data": "/content/drive/MyDrive/Data/Surface Temperature data.csv",
    "Wind Speeds Data": "/content/drive/MyDrive/Data/Wind_Speeds_Data.csv",
    "Humidity Data": "/content/drive/MyDrive/Data/humidity_data.csv",
    "TCWV Data": "/content/drive/MyDrive/Data/TCWV_data.csv",
    "solar_data": "/content/drive/MyDrive/Data/solar_data.csv"
    "atmospheric_data": "/content/drive/MyDrive/Data/atmospheric_data.csv"
   }

datasets = {name: pd.read_csv(path) for name, path in file_paths.items()}

# preprocess each dataframe
def preprocess_dataframe(df):
    df = df.drop_duplicates()
    df = df.fillna(method='ffill').fillna(method='bfill')  # Forward fill, then backward fill

    # convert appropriate data types
    for col in df.select_dtypes(include=['object']).columns:
        try:
            df[col] = pd.to_numeric(df[col])
        except ValueError:
            pass
    return df

# Apply preprocessing to all datasets
cleaned_datasets = {name: preprocess_dataframe(df) for name, df in datasets.items()}

import pandas as pd
# Load precipitation data
precipitation_data = pd.read_csv('/content/drive/MyDrive/Data/Mean precipitaion india.csv')

# Calculate monthly precipitation
monthly_precipitation = precipitation_data.iloc[:, 1:].sum(axis=0)

# Load wind speed data
wind_speed_data = pd.read_csv('/content/drive/MyDrive/Data/Wind_Speeds_Data.csv')

# Calculate average wind speed
average_wind_speed = wind_speed_data.iloc[:, 1:].mean(axis=1)

# Merge monthly precipitation and average wind speed with climatic data
climatic_data = pd.merge(temperature_data[['State', 'Avg_Temperature']],
                         pd.DataFrame({'Total_Precipitation': monthly_precipitation, 'State': precipitation_data['State']}), on='State')
climatic_data = pd.merge(climatic_data, pd.DataFrame({'Avg_Wind_Speed': average_wind_speed, 'State': wind_speed_data['State']}), on='State')

# Display the final climatic data
print("Climatic Data:")
print(climatic_data.head())

print(humidity_data.columns)
print(temperature_data.columns)
print(precipitation_data.columns)

# Renaming columns for consistency
humidity_data.rename(columns={'Name': 'State'}, inplace=True)
temperature_data.rename(columns={'Name': 'State'}, inplace=True)
precipitation_data.rename(columns={'STATE': 'State'}, inplace=True)

data = pd.merge(humidity_data, temperature_data,TCWV_data, Solar_radiation_data, Surface Temperature data  on='State')
data = pd.merge(data, precipitation_data, on='State')

# Load expanded atmospheric transmission data
transmission_data = pd.read_csv('expanded_transmission_data.csv')

# Extract wavelengths and transmission values
wavelengths = transmission_data['Wavelength (µm)'].values
tau_lambda = transmission_data['Transmission'].values
epsilon_atm = 1 - tau_lambda

!pip install poppler-utils

import numpy as np
from scipy.constants import h, c, k
from scipy.integrate import simps

A = 1.0  # Area in m^2
T = 300.0  # Temperature of the surface in K
T_amb = 295.0  # Ambient temperature in K
h_conv = 10.0  # Heat transfer coefficient in W/m^2K
R = 0.94 # Reflectivity
epsilon = 0.96 #Emissivity

# Blackbody radiation function
def blackbody_radiation(T, wavelength):
    return (2 * h * c**2 / wavelength**5) / (np.exp(h * c / (wavelength * k * T)) - 1)

# Integrate blackbody radiation over the spectrum using Simpson's rule
def integrate_blackbody(T, epsilon, wavelengths):
    radiation = epsilon * blackbody_radiation(T, wavelengths)
    integral = simps(radiation, wavelengths)
    return integral

# Power radiated by the surface (using wavelengths in meters)
P_rad = A * np.pi * integrate_blackbody(T, epsilon, atmospheric_data['wavelength'])

# Power absorbed from the sun
solar_irradiance_integrated = simps(solar_data['irradiance'], solar_data['wavelength'])
P_sun = A * (1 - R) * solar_irradiance_integrated

# Power absorbed from atmospheric radiation
P_atm = A * np.pi * integrate_blackbody(T_amb, epsilon_atm, atmospheric_data['wavelength'])

# Power lost due to conduction and convection
P_cond_conv = h_conv * A * (T - T_amb)

# Net cooling power
P_net = P_rad - P_sun - P_atm - P_cond_conv

print(f'Net Cooling Power: {P_net:.2f} W/m^2')

import geopandas as gpd
import matplotlib.pyplot as plt
import pandas as pd

# Load the India shapefile
shapefile_path = '/content/drive/MyDrive/Data/shapefiles/india_st.shp'
india_map = gpd.read_file(shapefile_path)

# Display the first few rows to verify
print(india_map.head(50))



import pandas as pd
climate_data=pd.read_csv('/content/drive/MyDrive/Data/climate_data.csv')
climate_data.head(40)

# Standardize region names
india_map['STATE'] = india_map['STATE'].str.strip().str.upper()
climatic_data['Region'] = climatic_data['Region'].str.strip().str.upper()

# Check unique region names
print("Unique region names in shapefile:")
print(india_map['STATE'].unique())

print("\nUnique region names in climatic data:")
print(climatic_data['Region'].unique())

# Correcting region names in climatic data to match those in the shapefile
region_corrections = {
    'ODISHA': 'ORISSA',
    'PUDUCHERRY': 'PONDICHERRY'
}

climatic_data['Region'] = climatic_data['Region'].replace(region_corrections)

missing_regions_in_shapefile = set(climatic_data['Region']).difference(set(india_map['STATE']))
print("Regions in climatic data not present in shapefile:", missing_regions_in_shapefile)

missing_regions_in_climatic_data = set(india_map['STATE']).difference(set(climatic_data['Region']))
print("Regions in shapefile not present in climatic data:", missing_regions_in_climatic_data)

# Now standardize names again after corrections
india_map['STATE'] = india_map['STATE'].str.strip().str.upper()
climatic_data['Region'] = climatic_data['Region'].str.strip().str.upper()
merged_data = india_map.set_index(india_map_column).join(climatic_data.set_index(climatic_data_column))

import matplotlib.pyplot as plt
import seaborn as sns

# Predict cooling power for the entire dataset
climatic_data['Predicted_Cooling_Power'] = model.predict(imputer.transform(climatic_data[['Avg_Temperature', 'Total_Precipitation', 'Avg_Humidity']]))

# Plot cooling power variations across regions
plt.figure(figsize=(12, 8))
sns.barplot(x='Region', y='Predicted_Cooling_Power', data=climatic_data)
plt.xticks(rotation=90)
plt.title('Predicted Cooling Power Variations Across Regions in India')
plt.xlabel('Region')
plt.ylabel('Predicted Cooling Power (W/m^2)')
plt.show()

# Distribution of Cooling Power
plt.figure(figsize=(10, 6))
sns.histplot(climatic_data['Cooling_Power'], kde=True, color='skyblue')
plt.title('Distribution of Cooling Power', fontsize=15)
plt.xlabel('Cooling Power (W/m^2)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.show()

!pip install seaborn
import seaborn as sns # import seaborn library

plt.figure(figsize=(16, 10))

# Line plot to show trend of cooling power across seasons for each state
data = pd.read_csv('/content/drive/MyDrive/Data/updated_merged_data.csv') # Load the data outside the loop

for season in seasons:
    sns.lineplot(data=data, x='STATE', y=f'{season}_Cooling_Power', label=season) # Pass the DataFrame to data

plt.xlabel('State')
plt.ylabel('Cooling Power (W/m^2)')
plt.title('Trend of Cooling Power Across Seasons for Each State')
plt.xticks(rotation=90)
plt.legend()
plt.show()

import matplotlib.pyplot as plt # Import matplotlib for plotting

plt.figure(figsize=(14, 10))

sns.boxplot(data=updated_merged_data[['Winter_Cooling_Power', 'Summer_Cooling_Power', 'Monsoon_Cooling_Power', 'Autumn_Cooling_Power']])
plt.title('Distribution of Cooling Power by Season')
plt.xlabel('Season')
plt.ylabel('Cooling Power (W/m^2)')
plt.show()

# Temporal Analysis

seasons = ['Winter', 'Summer', 'Monsoon', 'Autumn']
cooling_power_cols = [f'{season}_Cooling_Power' for season in seasons]
temp_cols = [f'{season}_Temperature' for season in seasons]
solar_rad_cols = [f'{season}_Solar_Radiation' for season in seasons]
tcwv_cols = [f'{season}_TCWV' for season in seasons]

def plot_seasonal_data(data, columns, title):
    plt.figure(figsize=(12, 6))
    data[columns].mean().plot(marker='o')
    plt.title(title)
    plt.ylabel('Average Value')
    plt.xlabel('Season')
    plt.show()

plot_seasonal_data(df, cooling_power_cols, 'Average Cooling Power by Season')
plot_seasonal_data(df, temp_cols, 'Average Temperature by Season')
plot_seasonal_data(df, solar_rad_cols, 'Average Solar Radiation by Season')
plot_seasonal_data(df, tcwv_cols, 'Average TCWV by Season')

# Correlation Analysis

correlation_matrix = df[cooling_power_cols + temp_cols + solar_rad_cols + tcwv_cols].corr()
plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix: Cooling Power, Temperature, Solar Radiation, and TCWV')
plt.tight_layout()
plt.show()

# Disorder Analysis

# Calculate variance in cooling power across seasons as a measure of disorder
df['Cooling_Power_Variance'] = df[cooling_power_cols].var(axis=1)

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Avg_Temperature', y='Cooling_Power_Variance', hue='Avg_TCWV', palette='viridis')
plt.title('Disorder (Variance in Cooling Power) vs Average Temperature, colored by Avg TCWV')
plt.xlabel('Average Temperature')
plt.ylabel('Cooling Power Variance (Disorder)')
plt.show()

# Efficiency Analysis

df['Cooling_Efficiency'] = df[cooling_power_cols].mean(axis=1) / (df['Avg_Temperature'] * df['Avg_TCWV'])

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Cooling_Power_Variance', y='Cooling_Efficiency', hue='Avg_TCWV', palette='viridis')
plt.title('Cooling Efficiency vs Disorder, colored by Avg TCWV')
plt.xlabel('Cooling Power Variance (Disorder)')
plt.ylabel('Cooling Efficiency')
plt.show()



# Clustering Analysis

features = cooling_power_cols + temp_cols + solar_rad_cols + tcwv_cols
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[features])

kmeans = KMeans(n_clusters=5, random_state=42)
df['Cluster'] = kmeans.fit_predict(scaled_data)

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Avg_Temperature', y='Cooling_Power_Variance', hue='Cluster', palette='viridis')
plt.title('Disorder (Variance in Cooling Power) vs Average Temperature, colored by Cluster')
plt.xlabel('Average Temperature')
plt.ylabel('Cooling Power Variance (Disorder)')
plt.show()

print(df.index)
print(df['Radiative_Cooling_Potential'].dtype)
print(df.loc[:, 'Radiative_Cooling_Potential'].head())

india_map = gpd.read_file('/content/drive/MyDrive/Data/shapefiles/india_ds.shp')
print(india_map.columns)
print(india_map.info())

print(sorted(india_map['STATE'].unique()))

map=gpd.read_file('/content/drive/MyDrive/Data/shapefiles/india_st.shp')
print(map.columns)
print(map.info())
for row in map.itertuples():
  print(row.geometry)



from shapely.geometry import Point

merged_data = map_df.merge(df, left_on='STATE', right_on='STATE', how='left')

fig, axes = plt.subplots(3, 2, figsize=(20, 30))

merged_data.plot(column='Avg_Temperature', cmap='YlOrRd', linewidth=0.8, edgecolor='0.8', ax=axes[0, 0], legend=True, legend_kwds={'label': 'Average Temperature (°C)', 'orientation': 'horizontal'})
axes[0, 0].set_title('Average Temperature Across Indian States')
axes[0, 0].axis('off')

merged_data.plot(column='Avg_Precipitation', cmap='Blues', linewidth=0.8, edgecolor='0.8', ax=axes[0, 1], legend=True, legend_kwds={'label': 'Total Precipitation (mm)', 'orientation': 'horizontal'})
axes[0, 1].set_title('Average Precipitation Across Indian States')
axes[0, 1].axis('off')

merged_data.plot(column='Avg_Solar_Radiation', cmap='YlOrBr', linewidth=0.8, edgecolor='0.8', ax=axes[1, 0], legend=True, legend_kwds={'label': 'Average Solar Radiation', 'orientation': 'horizontal'})
axes[1, 0].set_title('Average Solar Radiation Across Indian States')
axes[1, 0].axis('off')

merged_data.plot(column='Avg_Wind_Speed', cmap='Greens', linewidth=0.8, edgecolor='0.8', ax=axes[1, 1], legend=True, legend_kwds={'label': 'Average Wind Speed', 'orientation': 'horizontal'})
axes[1, 1].set_title('Average Wind Speed Across Indian States')
axes[1, 1].axis('off')

merged_data.plot(column='Avg_Humidity', cmap='PuBu', linewidth=0.8, edgecolor='0.8', ax=axes[2, 0], legend=True, legend_kwds={'label': 'Average Humidity', 'orientation': 'horizontal'})
axes[2, 0].set_title('Average Humidity Across Indian States')
axes[2, 0].axis('off')

merged_data.plot(column='Avg_TCWV', cmap='Oranges', linewidth=0.8, edgecolor='0.8', ax=axes[2, 1], legend=True, legend_kwds={'label': 'Average TCWV', 'orientation': 'horizontal'})
axes[2, 1].set_title('Average TCWV Across Indian States')
axes[2, 1].axis('off')

plt.tight_layout()
plt.show()

# Geospatial Analysis

def plot_choropleth(data, column, title):
    fig, ax = plt.subplots(1, 1, figsize=(15, 10))
    data.plot(column=column, ax=ax, legend=True, cmap='YlOrRd',
              legend_kwds={'label': column, 'orientation': 'horizontal'})
    ax.set_title(title)
    ax.axis('off')
    plt.tight_layout()
    plt.show()

plot_choropleth(merged_data, 'Cooling_Efficiency', 'Cooling Efficiency Across India')
plot_choropleth(merged_data, 'Cooling_Power_Variance', 'Cooling Power Variance (Disorder) Across India')
plot_choropleth(merged_data, 'Avg_TCWV', 'Average TCWV Across India')

seasons = ['Winter', 'Summer', 'Monsoon', 'Autumn']
cooling_power_cols = [f'{season}_Cooling_Power' for season in seasons]
temp_cols = [f'{season}_Temperature' for season in seasons]
solar_rad_cols = [f'{season}_Solar_Radiation' for season in seasons]
tcwv_cols = [f'{season}_TCWV' for season in seasons]

def plot_seasonal_data(data, columns, title):
    plt.figure(figsize=(12, 6))
    data[columns].mean().plot(kind='bar')
    plt.title(title)
    plt.ylabel('Average Value')
    plt.xlabel('Season')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_seasonal_data(df, cooling_power_cols, 'Average Cooling Power by Season')
plot_seasonal_data(df, temp_cols, 'Average Temperature by Season')
plot_seasonal_data(df, solar_rad_cols, 'Average Solar Radiation by Season')
plot_seasonal_data(df, tcwv_cols, 'Average Total Column Water Vapor (TCWV) by Season')

df['Cooling_Power_Variance'] = df[cooling_power_cols].var(axis=1)
df['Cooling_Efficiency'] = df['Avg_Cooling_Power'] / df['Avg_Temperature']

features = cooling_power_cols + temp_cols + solar_rad_cols + tcwv_cols
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[features])

kmeans = KMeans(n_clusters=5, random_state=42)
df['Cluster'] = kmeans.fit_predict(scaled_data)

# Temporal Trend Analysis

df['Cooling_Power_Trend'] = df[cooling_power_cols].apply(lambda x: stats.linregress(range(4), x).slope, axis=1)

plt.figure(figsize=(10, 6))
sns.histplot(df['Cooling_Power_Trend'], kde=True)
plt.title('Distribution of Cooling Power Trends')
plt.xlabel('Trend Slope')
plt.show()

# Radiative Cooling Potential Index

df['Radiative_Cooling_Potential'] = (
    df['Avg_Solar_Radiation'] * 0.3 +
    df['Cooling_Efficiency'] * 0.25 +
    (1 / df['Cooling_Power_Variance']) * 0.2 +
    df['Avg_Cooling_Power'] * 0.15 +
    (1 / df['Avg_TCWV']) * 0.1
)

# Normalize the index
df['Radiative_Cooling_Potential'] = (df['Radiative_Cooling_Potential'] - df['Radiative_Cooling_Potential'].min()) / (df['Radiative_Cooling_Potential'].max() - df['Radiative_Cooling_Potential'].min())

import matplotlib.pyplot as plt
sorted_data = df.sort_values('Radiative_Cooling_Potential', ascending=False)

plt.figure(figsize=(15, 10))
sns.barplot(x=sorted_data.index, y='Radiative_Cooling_Potential', data=sorted_data)
plt.title('Radiative Cooling Potential by State (Descending)')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

seasonal_variation = df[cooling_power_cols].std(axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(seasonal_variation, kde=True)
plt.title('Distribution of Seasonal Variation in Cooling Power')
plt.xlabel('Standard Deviation of Cooling Power Across Seasons')
plt.show()

# Cluster Characteristics

cluster_means = df.groupby('Cluster')[features + ['Radiative_Cooling_Potential']].mean()
print("\nCluster Characteristics:")
print(cluster_means)

#Efficiency vs. Disorder Trend

plt.figure(figsize=(10, 6))
sns.regplot(data=df, x='Cooling_Power_Variance', y='Cooling_Efficiency')
plt.title('Trend: Cooling Efficiency vs Disorder')
plt.xlabel('Cooling Power Variance (Disorder)')
plt.ylabel('Cooling Efficiency')
plt.show()

!pip install pysal

import matplotlib.pyplot as plt
plt.figure(figsize=(14, 10))
sns.set(style="whitegrid", font_scale=1.2)

scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=df['Cluster'], cmap='viridis', s=70)

plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance explained)', fontsize=14)
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance explained)', fontsize=14)
plt.title('PCA of Climate Data for Indian States', fontsize=16)

for i, state in enumerate(df['STATE']):
    if state in ['KERALA', 'RAJASTHAN', 'ASSAM', 'JAMMU AND KASHMIR', 'TAMIL NADU']:
        plt.annotate(state, (pca_result[i, 0], pca_result[i, 1]),
                     xytext=(5, 5), textcoords='offset points',
                     fontsize=12, fontweight='bold')

cluster_descriptions = [
    'Hot, dry states',
    'Cooler, wetter states',
    'Moderate climate states',
    'High-altitude regions',
    'Coastal, high precipitation states'
]
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=desc,
                   markerfacecolor=scatter.cmap(scatter.norm(i)), markersize=10)
                   for i, desc in enumerate(cluster_descriptions)]
plt.legend(handles=legend_elements, title='Cluster Descriptions',
           loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12, title_fontsize=14)

plt.tight_layout()
plt.show()

pip install pysal



from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats
from statsmodels.tsa.seasonal import seasonal_decompose
from pysal.explore import esda
from pysal.lib import weights
import folium


if map_df.crs is None:
    map_df.set_crs(epsg=4326, inplace=True)  # WGS84 coordinate system

merged_data = map_df.merge(df, left_on='STATE', right_on='STATE', how='left')

# 3. Random Forest Regression
X = df[features_for_clustering]
y = df['Avg_Cooling_Power']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Random Forest Regression Results:")
print(f"Mean Squared Error: {mse}")
print(f"R-squared Score: {r2}")


# Spatial Autocorrelation (Moran's I)
w = weights.distance.KNN.from_dataframe(merged_data, k=5)
w.transform = 'R'

moran = esda.Moran(merged_data['Avg_Temperature'], w)
print(f"Moran's I: {moran.I}")
print(f"p-value: {moran.p_sim}")

# Time Series Analysis (assuming we have seasonal data for a year)
seasonal_cols = ['Winter_Temperature', 'Summer_Temperature', 'Monsoon_Temperature', 'Autumn_Temperature']
time_series = df[seasonal_cols].melt(var_name='Season', value_name='Temperature')
time_series['Date'] = pd.date_range(start='2023-01-01', periods=len(time_series), freq='QS')
time_series = time_series.set_index('Date')

result = seasonal_decompose(time_series['Temperature'], model='additive', period=4)
result.plot()
plt.show()

# Geospatial Analysis with Folium
m = folium.Map(location=[20.5937, 78.9629], zoom_start=4)

folium.Choropleth(
    geo_data=merged_data.__geo_interface__,  # Convert to geo_interface
    name='Avg Temperature',
    data=merged_data,
    columns=['STATE', 'Avg_Temperature'],
    key_on='feature.properties.STATE',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Average Temperature (°C)'
).add_to(m)

folium.LayerControl().add_to(m)
m.save('/content/drive/MyDrive/Data/folium_map.html')



atmospheric_data['emissivity'] = 1 - atmospheric_data['transmission']
avg_atm_emissivity = np.mean(atmospheric_data['emissivity'])

def blackbody_radiation(T, wavelength):
    return (2 * h * c**2 / wavelength**5) / (np.exp(h * c / (wavelength * k * T)) - 1)

def integrate_blackbody(T, epsilon, wavelengths):
    radiation = epsilon * blackbody_radiation(T, wavelengths)
    integral = simps(radiation, wavelengths)
    return integral

def calculate_cooling_power(T, T_amb, solar_radiation, tcwv, season):
    T_kelvin = T + 273.15
    T_amb_kelvin = T_amb + 273.15
    epsilon_m = 0.96
    reflectance_m = 0.94
    # Adjust atmospheric emissivity based on TCWV (refined approximation)
    epsilon_atm = 0.72 + 0.28 * np.tanh((tcwv - 6000) / 1000)

    P_rad = A * np.pi * integrate_blackbody(T_kelvin, epsilon_m, atmospheric_data['wavelength'])
    P_sun = A * (1 - reflectance_m) * solar_radiation
    P_atm = A * np.pi * integrate_blackbody(T_amb_kelvin, epsilon_atm, atmospheric_data['wavelength'])
    P_cond_conv = h_conv * A * (T - T_amb)
    P_net = P_rad - (P_sun + P_atm + P_cond_conv)
    return P_net, P_rad, P_sun, P_atm, P_cond_conv

cooling_power_data = []

# Iterate through each state and calculate cooling power for each season
for state in climatic_data['STATE'].unique():
    state_data = climatic_data[climatic_data['STATE'] == state]
    state_cooling_power = {'state': state}

    seasonal_powers = []

    for season in ['Winter', 'Summer', 'Monsoon', 'Autumn']:
        temp_col = f'{season}_Temperature'
        solar_col = f'{season}_Solar_Radiation'
        tcwv_col = f'{season}_TCWV'

        T = state_data.iloc[0][temp_col]
        T_amb = state_data.iloc[0]['Avg_Temperature']
        solar_radiance = state_data.iloc[0][solar_col]
        tcwv = state_data.iloc[0][tcwv_col]

        P_net, P_rad, P_sun, P_atm, P_cond_conv = calculate_cooling_power(T, T_amb, solar_radiance, tcwv, season)
        state_cooling_power[f'{season}_Cooling_Power'] = P_net
        state_cooling_power[f'{season}_P_rad'] = P_rad
        state_cooling_power[f'{season}_P_sun'] = P_sun
        state_cooling_power[f'{season}_P_atm'] = P_atm
        state_cooling_power[f'{season}_P_cond_conv'] = P_cond_conv
        seasonal_powers.append(P_net)

    state_cooling_power['Avg_Cooling_Power'] = np.mean(seasonal_powers)
    cooling_power_data.append(state_cooling_power)

cooling_power_df = pd.DataFrame(cooling_power_data)

cooling_power_melted = cooling_power_df.melt(id_vars=['state'],
                                             value_vars=['Winter_Cooling_Power', 'Summer_Cooling_Power',
                                                         'Monsoon_Cooling_Power', 'Autumn_Cooling_Power', 'Avg_Cooling_Power'],
                                             var_name='Season',
                                             value_name='Cooling_Power')

plt.figure(figsize=(14, 8))
barplot = sns.barplot(data=cooling_power_melted, x='state', y='Cooling_Power', hue='Season')
barplot.set_xticklabels(barplot.get_xticklabels(), rotation=90)
plt.title('Cooling Power for Different Seasons Across States')
plt.xlabel('State')
plt.ylabel('Cooling Power (W/m^2)')
plt.legend(title='Season', loc='upper right')
plt.tight_layout()
plt.show()

# Display the cooling power data with components
print(cooling_power_df)

merged_gdf = shapefile.merge(data, left_on='STATE', right_on='STATE')

# Create a figure with 4 subplots for each season
fig, axes = plt.subplots(2, 2, figsize=(20, 15))

# Loop through each season and plot the choropleth map
seasons = ['Winter', 'Summer', 'Monsoon', 'Autumn']
for i, season in enumerate(seasons):
    ax = axes[i // 2, i % 2]
    merged_gdf.plot(column=f'{season}_Cooling_Power', ax=ax, legend=True, cmap='coolwarm')
    ax.set_title(f'{season} Cooling Power Distribution Across States')

plt.tight_layout()
plt.show()

def prepare_data_for_boxplots(df):
    cooling_power_data = df[['STATE', 'Winter_Cooling_Power', 'Summer_Cooling_Power',
                             'Monsoon_Cooling_Power', 'Autumn_Cooling_Power', 'Avg_Cooling_Power']]
    cooling_power_melted = cooling_power_data.melt(id_vars=['STATE'],
                                                   value_vars=['Winter_Cooling_Power', 'Summer_Cooling_Power',
                                                               'Monsoon_Cooling_Power', 'Autumn_Cooling_Power', 'Avg_Cooling_Power'],
                                                   var_name='Season',
                                                   value_name='Cooling_Power')
    return cooling_power_melted

cooling_power_melted = prepare_data_for_boxplots(climatic_data)

plt.figure(figsize=(14, 8))
boxplot = sns.boxplot(data=cooling_power_melted, x='Season', y='Cooling_Power')
plt.title('Boxplots of Cooling Power Across Seasons')
plt.xlabel('Season')
plt.ylabel('Cooling Power (W/m^2)')
plt.tight_layout()
plt.show()

import pandas as pd
import geopandas as gpd
import plotly.graph_objects as go

shapefile_path = 'india_st.shp'
gdf = gpd.read_file(shapefile_path)
gdf['STATE'] = gdf['STATE'].str.upper()

# Convert to GeoJSON
geojson = gdf.__geo_interface__

def clean_columns(df):
    df.columns = df.columns.str.strip().str.replace(r'[\r\n]', '', regex=True)
    return df

climatic_data = clean_columns(climatic_data)

# Prepare data for Plotly
plotly_data = climatic_data.melt(id_vars=['STATE'], value_vars=['Avg_Cooling_Power', 'Winter_Cooling_Power', 'Summer_Cooling_Power', 'Monsoon_Cooling_Power', 'Autumn_Cooling_Power'], var_name='Season', value_name='Cooling_Power')

frames = []
seasons = ['Avg_Cooling_Power', 'Winter_Cooling_Power', 'Summer_Cooling_Power', 'Monsoon_Cooling_Power', 'Autumn_Cooling_Power']
for season in seasons:
    frame = go.Frame(
        data=[go.Choropleth(
            locations=plotly_data['STATE'][plotly_data['Season'] == season],
            z=plotly_data['Cooling_Power'][plotly_data['Season'] == season],
            geojson=geojson,
            featureidkey="properties.STATE",
            colorscale="Viridis",
            hoverinfo='location+z'
        )],
        name=season
    )
    frames.append(frame)

fig = go.Figure(
    data=[go.Choropleth(
        locations=plotly_data['STATE'][plotly_data['Season'] == 'Avg_Cooling_Power'],
        z=plotly_data['Cooling_Power'][plotly_data['Season'] == 'Avg_Cooling_Power'],
        geojson=geojson,
        featureidkey="properties.STATE",
        colorscale="Viridis",
        hoverinfo='location+z'
    )],
    frames=frames
)

# adding buttons for changing seasons
fig.update_layout(
    updatemenus=[
        {
            'buttons': [
                {
                    'args': [[season], {'frame': {'duration': 500, 'redraw': True}, 'mode': 'immediate'}],
                    'label': season.replace('_Cooling_Power', ''),
                    'method': 'animate'
                } for season in seasons
            ],
            'direction': 'down',
            'showactive': True,
        },
        {
            'buttons': [
                {
                    'args': [None, {'frame': {'duration': 1000, 'redraw': True}, 'fromcurrent': True, 'mode': 'immediate'}],
                    'label': 'Play',
                    'method': 'animate'
                },
                {
                    'args': [[None], {'frame': {'duration': 0, 'redraw': True}, 'mode': 'immediate'}],
                    'label': 'Pause',
                    'method': 'animate'
                }
            ],
            'direction': 'left',
            'showactive': True,
            'x': 0.1,
            'xanchor': 'right',
            'y': 1.1,
            'yanchor': 'top'
        },
        {
            'buttons': [
                {
                    'args': ['colorscale', 'Viridis'],
                    'label': 'Viridis',
                    'method': 'restyle'
                },
                {
                    'args': ['colorscale', 'Cividis'],
                    'label': 'Cividis',
                    'method': 'restyle'
                },
                {
                    'args': ['colorscale', 'Bluered'],
                    'label': 'Bluered',
                    'method': 'restyle'
                }
            ],
            'direction': 'down',
            'showactive': True,
            'x': 0.1,
            'xanchor': 'right',
            'y': 1,
            'yanchor': 'top'
        }
    ]
)

fig.update_geos(fitbounds="locations", visible=False)
fig.update_layout(title_text='Geospatial Distribution of Cooling Power in India')

fig.show()

from ipywidgets import interact, widgets

# Interactive function
def update_plot( epsilon=0.96, R=0.94):
    cooling_power_data = []

    for state in climatic_data['STATE'].unique():
        state_data = climatic_data[climatic_data['STATE'] == state]
        state_cooling_power = {'STATE': state}

        for season in ['Winter', 'Summer', 'Monsoon', 'Autumn']:
            temp_col = f'{season}_Temperature'
            solar_col = f'{season}_Solar_Radiation'
            tcwv_col = f'{season}_TCWV'

            T = state_data.iloc[0][temp_col]
            T_amb = state_data.iloc[0]['Avg_Temperature']
            solar_radiance = state_data.iloc[0][solar_col]
            tcwv = state_data.iloc[0][tcwv_col]

            cooling_power = calculate_cooling_power(T, T_amb, solar_radiance, h_conv, epsilon, R, tcwv, season)
            state_cooling_power[f'{season}_Cooling_Power'] = cooling_power

        cooling_power_data.append(state_cooling_power)

    cooling_power_df = pd.DataFrame(cooling_power_data)

    cooling_power_melted = cooling_power_df.melt(id_vars=['STATE'],
                                                 value_vars=['Winter_Cooling_Power', 'Summer_Cooling_Power',
                                                             'Monsoon_Cooling_Power', 'Autumn_Cooling_Power'],
                                                 var_name='Season',
                                                 value_name='Cooling_Power')

    plt.figure(figsize=(14, 8))
    barplot = sns.barplot(data=cooling_power_melted, x='STATE', y='Cooling_Power', hue='Season')
    barplot.set_xticklabels(barplot.get_xticklabels(), rotation=90)
    plt.title('Cooling Power for Different Seasons Across States')
    plt.xlabel('State')
    plt.ylabel('Cooling Power (W/m^2)')
    plt.legend(title='Season', loc='upper right')
    plt.tight_layout()
    plt.show()


interact(update_plot,
         epsilon=widgets.FloatSlider(min=0.5, max=1.0, step=0.01, value=0.96, description='Emissivity:'),
         R=widgets.FloatSlider(min=0.5, max=1.0, step=0.01, value=0.94, description='Reflectivity:'));

!pip install tensorflow

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

def clean_columns(df):
    df.columns = df.columns.str.strip().str.replace(r'[\r\n]', '', regex=True)
    return df
data = clean_columns(data)

features = [ 'Avg_Temperature', 'Avg_Precipitation', 'Avg_Solar_Radiation', 'Avg_Wind_Speed', 'Avg_Humidity','Avg_TCWV']

target =[ 'Avg_Cooling_Power']
X = data[features]
y = data[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

def evaluate_model(y_true, y_pred, model_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    print(f"{model_name} - RMSE: {rmse:.4f}, R2: {r2:.4f}")
    return rmse, r2

selector = SelectFromModel(estimator=RandomForestRegressor(n_estimators=100, random_state=42))
selector.fit(X_train_scaled, y_train)
X_train_selected = selector.transform(X_train_scaled)
X_test_selected = selector.transform(X_test_scaled)


# Random Forest Model (Add this section)
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
rf_pred = rf_model.predict(X_test_scaled)
rf_rmse, rf_r2 = evaluate_model(y_test, rf_pred, "Random Forest")

# XGBoost Model (Add this section if you want to compare with XGBoost)
xgb_model = XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_rmse, xgb_r2 = evaluate_model(y_test, xgb_pred, "XGBoost")

import pandas as pd
# Support Vector Machine
from sklearn.svm import SVR

svm_model = SVR(kernel='rbf', C=100, gamma=0.1)
svm_model.fit(X_train_scaled, y_train)
svm_pred = svm_model.predict(X_test_scaled)
svm_rmse, svm_r2 = evaluate_model(y_test, svm_pred, "Support Vector Machine")

# K-Nearest Neighbors
from sklearn.neighbors import KNeighborsRegressor

knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)
knn_pred = knn_model.predict(X_test_scaled)
knn_rmse, knn_r2 = evaluate_model(y_test, knn_pred, "K-Nearest Neighbors")

# Decision Tree
from sklearn.tree import DecisionTreeRegressor

dt_model = DecisionTreeRegressor(max_depth=5)
dt_model.fit(X_train_scaled, y_train)
dt_pred = dt_model.predict(X_test_scaled)
dt_rmse, dt_r2 = evaluate_model(y_test, dt_pred, "Decision Tree")

# Gradient Boosting
from sklearn.ensemble import GradientBoostingRegressor

gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)
gb_model.fit(X_train_scaled, y_train)
gb_pred = gb_model.predict(X_test_scaled)
gb_rmse, gb_r2 = evaluate_model(y_test, gb_pred, "Gradient Boosting")

import pandas as pd
from sklearn.linear_model import LinearRegression

# Create a linear regression model
lr_model = LinearRegression()

# Fit the model to the training data
lr_model.fit(X_train_scaled, y_train)

# Predict on the test data
lr_pred = lr_model.predict(X_test_scaled)

# Evaluate the model
lr_rmse, lr_r2 = evaluate_model(y_test, lr_pred, "Linear Regression")



# Compare all models
results = pd.DataFrame({
    'Model': ['Random Forest', 'XGBoost', 'SVM', 'Decision Tree', 'Gradient Boosting', 'Linear Regression'],
    'RMSE': [rf_rmse, xgb_rmse, svm_rmse, dt_rmse, gb_rmse, lr_rmse],
    'R2': [rf_r2, xgb_r2, svm_r2, dt_r2, gb_r2, lr_r2]
})
print("\nModel Comparison:")
print(results.sort_values('RMSE'))